{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gabri\\\\OneDrive\\\\Área de Trabalho\\\\Database_code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KT20 cond and mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_sus_proc(drillhole,prop):\n",
    "\n",
    "    data_inventory = pd.read_excel(f'{drillhole}/{drillhole}_sample_report.xlsx', sheet_name = None)\n",
    "    df_record = data_inventory.get(prop)\n",
    "\n",
    "    os.chdir(f'{drillhole}/cond_sus')\n",
    "    \n",
    "\n",
    "    dic_result = {'sample id':[],'mean':[], 'std':[],'median':[], 'max':[],'min':[]} ## creating dataframe for results #######\n",
    "\n",
    "\n",
    "    for dir in os.listdir():\n",
    "        os.chdir(dir+'/'+'kt20_0385')\n",
    "\n",
    "        try:\n",
    "            meta_data = pd.read_csv('kappa_sigma_0385.csv')\n",
    "            df_record_sub = df_record[df_record['measurement_id'].isin(meta_data['Record_Id'])]\n",
    "            \n",
    "        except KeyError:\n",
    "            meta_data = pd.read_csv('kappa_sigma_0385.csv',sep=';')\n",
    "            df_record_sub = df_record[df_record['measurement_id'].isin(meta_data['Record_Id'])]\n",
    "\n",
    "                                  \n",
    "        record_id_list_sub = df_record_sub['measurement_id'].tolist()\n",
    "        record_id_strings_sub = [str(record_id) for record_id in record_id_list_sub] \n",
    "        sample_id_sub = df_record_sub['sample_id'].tolist()\n",
    "        sample_id_strings_sub = [str(sample_id) for sample_id in sample_id_sub]\n",
    "\n",
    "        for i in range(len(record_id_strings_sub)):\n",
    "            os.chdir(str(record_id_strings_sub[i]))   #### changing directory iteratively##########\n",
    "            file = glob.glob('*.csv')\n",
    "            df = pd.read_csv(file[0])    \n",
    "            a1 = df.index[df['Date'] == 'Min'].to_list()\n",
    "            a1 = int(np.array(a1))\n",
    "            d1 = np.array(df.iloc[2:a1,1])\n",
    "            d1 = d1.astype('float32')\n",
    "            d1 = np.reshape(d1, (a1-2,1))\n",
    "            w1 = np.ma.masked_outside(d1,np.mean(d1)+(2*np.std(d1)),np.mean(d1)-(2*np.std(d1)))\n",
    "            dic_result['sample id'].append(sample_id_strings_sub[i]),dic_result['mean'].append(np.mean(w1)),dic_result['std'].append(np.std(w1)),dic_result['median'].append(np.median(w1)),dic_result['max'].append(np.max(w1)),dic_result['min'].append(np.min(w1))\n",
    "            os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir))\n",
    "            # print(os.getcwd())\n",
    "        os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir+os.sep + os.pardir))\n",
    "\n",
    "    os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir + os.sep + os.pardir))\n",
    "\n",
    "    df_proc = pd.DataFrame(dic_result)\n",
    "\n",
    "    sample_id = df_record['sample_id']\n",
    "    nan_indices = df_record[df_record['measurement_id'].isna()].index\n",
    "\n",
    "    dic_nan = {str(i) : ['nan'] for i in df_proc.columns}\n",
    "    df_nan = pd.DataFrame(dic_nan)\n",
    "\n",
    "    \n",
    "    for index in nan_indices:\n",
    "        df_proc = pd.concat([df_proc.iloc[:index], df_nan, df_proc.iloc[index:]]).reset_index(drop=True)\n",
    "    result_df = df_proc.reset_index(drop=True)\n",
    "\n",
    "    result_df.iloc[:,0] = (sample_id).astype(str)\n",
    "    cal = result_df['sample id'].str.contains('cal')\n",
    "    df_cal = result_df[cal]\n",
    "    print(cal)\n",
    "    df_final = result_df[np.logical_not(cal)]\n",
    "    df_final.index = range(len(df_final))\n",
    "\n",
    "    return df_final,df_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'c:\\\\Users\\\\gabri\\\\OneDrive\\\\Área de Trabalho\\\\campaing03\\\\campaing03_samplereport.xlsx/c:\\\\Users\\\\gabri\\\\OneDrive\\\\Área de Trabalho\\\\campaing03\\\\campaing03_samplereport.xlsx_sample_report.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabri\\OneDrive\\Área de Trabalho\\Database_code\\data_processing.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive/%C3%81rea%20de%20Trabalho/Database_code/data_processing.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m _,t \u001b[39m=\u001b[39m cond_sus_proc(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mos\u001b[39m.\u001b[39;49mgetcwd()\u001b[39m}\u001b[39;49;00m\u001b[39m\\\u001b[39;49m\u001b[39mcampaing03\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcampaing03_samplereport.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39msus\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\gabri\\OneDrive\\Área de Trabalho\\Database_code\\data_processing.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive/%C3%81rea%20de%20Trabalho/Database_code/data_processing.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcond_sus_proc\u001b[39m(drillhole,prop):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive/%C3%81rea%20de%20Trabalho/Database_code/data_processing.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     data_inventory \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdrillhole\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdrillhole\u001b[39m}\u001b[39;49;00m\u001b[39m_sample_report.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m, sheet_name \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive/%C3%81rea%20de%20Trabalho/Database_code/data_processing.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df_record \u001b[39m=\u001b[39m data_inventory\u001b[39m.\u001b[39mget(prop)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive/%C3%81rea%20de%20Trabalho/Database_code/data_processing.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     os\u001b[39m.\u001b[39mchdir(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdrillhole\u001b[39m}\u001b[39;00m\u001b[39m/cond_sus\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1653\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1654\u001b[0m     )\n\u001b[0;32m   1655\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1526\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1527\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'c:\\\\Users\\\\gabri\\\\OneDrive\\\\Área de Trabalho\\\\campaing03\\\\campaing03_samplereport.xlsx/c:\\\\Users\\\\gabri\\\\OneDrive\\\\Área de Trabalho\\\\campaing03\\\\campaing03_samplereport.xlsx_sample_report.xlsx'"
     ]
    }
   ],
   "source": [
    "_,t = cond_sus_proc(f'{os.getcwd()}\\campaing03\\campaing03_samplereport.xlsx','sus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(drillhole):     \n",
    "    \n",
    "    dict = pd.read_excel(f'{drillhole}/{drillhole}_sample_report.xlsx', sheet_name = None)\n",
    "    df_record = dict.get('gamma') \n",
    "    \n",
    "    os.chdir(f'{drillhole}/gamma')\n",
    "\n",
    "    # measurement_id_array = np.c_[df_record['measurement_id1'],df_record['measurement_id2'],df_record['measurement_id3']]\n",
    "    dic_result = {'sample id':[],'K_mean':[],'K_median':[], 'K_max':[],'K_min':[], 'K_std':[],'U_mean':[],'U_median':[], \n",
    "                  'U_max':[],'U_min':[], 'U_std':[],'Th_mean':[],'Th_median':[], 'Th_max':[],'Th_min':[], 'Th_std':[]}\n",
    "\n",
    "    for dir in os.listdir():\n",
    "        os.chdir(f'{dir}/gt40_0154/assay')\n",
    "\n",
    "        # file_record = glob.glob('*.xlsx')\n",
    "        # df_record = pd.read_excel(file_record[0])\n",
    "\n",
    "        file_assay = glob.glob('*.csv')\n",
    "        assay = pd.read_csv(file_assay[0],sep=';')\n",
    "        df_assay = assay[assay['Id'].notna()]\n",
    "        df_record_sub = df_record[df_record['measurement_id1'].isin(df_assay['Id']) + \n",
    "                                  df_record['measurement_id2'].isin(df_assay['Id']) + df_record['measurement_id3'].isin(df_assay['Id'])]\n",
    "        sampleid = np.array(df_record_sub['sample_id'])\n",
    "        measurement_id_array = np.c_[df_record_sub['measurement_id1'],df_record_sub['measurement_id2'],df_record_sub['measurement_id3']] \n",
    "\n",
    "        \n",
    "        for i in range (len(df_record_sub)):\n",
    "            measurement_id_temp = measurement_id_array[i,:].tolist()\n",
    "            df_temp = df_assay[df_assay['Id'].isin(measurement_id_temp)]\n",
    "            df_temp.replace('<  0.00000', '0', inplace=True)\n",
    "            dic_result['sample id'].append(str(sampleid[i])),dic_result['K_mean'].append(df_temp['Component_1 value'].astype('float16').mean()),\n",
    "            dic_result['K_median'].append(df_temp['Component_1 value'].astype('float16').median()),dic_result['K_max'].append(df_temp['Component_1 value'].astype('float16').max()),\n",
    "            dic_result['K_min'].append(df_temp['Component_1 value'].astype('float16').min()),dic_result['K_std'].append(df_temp['Component_1 value'].astype('float16').std()),\n",
    "            dic_result['U_mean'].append(df_temp['Component_2 value'].astype('float16').mean()),dic_result['U_median'].append(df_temp['Component_2 value'].astype('float16').median()),\n",
    "            dic_result['U_max'].append(df_temp['Component_2 value'].astype('float16').max()),dic_result['U_min'].append(df_temp['Component_2 value'].astype('float16').min()),\n",
    "            dic_result['U_std'].append(df_temp['Component_2 value'].astype('float16').std()),\n",
    "            dic_result['Th_mean'].append(df_temp['Component_3 value'].astype('float16').mean()),dic_result['Th_median'].append(df_temp['Component_3 value'].astype('float16').median()),\n",
    "            dic_result['Th_max'].append(df_temp['Component_3 value'].astype('float16').max()),dic_result['Th_min'].append(df_temp['Component_3 value'].astype('float16').min()),dic_result['Th_std'].append(df_temp['Component_3 value'].astype('float16').std())\n",
    "\n",
    "       \n",
    "\n",
    "        os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir + os.sep + os.pardir + os.sep + os.pardir))   \n",
    "\n",
    "    os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir+os.sep + os.pardir))\n",
    "\n",
    "    df_proc = pd.DataFrame(dic_result)\n",
    "\n",
    "    sample_id = df_record['sample_id']\n",
    "    nan_indices = df_record[df_record['measurement_id1'].isna()].index\n",
    "    dic_nan = {str(i) : ['nan'] for i in df_proc.columns}\n",
    "    df_nan = pd.DataFrame(dic_nan)\n",
    "\n",
    "    for index in nan_indices:\n",
    "        df_proc = pd.concat([df_proc.iloc[:index], df_nan, df_proc.iloc[index:]]).reset_index(drop=True)\n",
    "    result_df = df_proc.reset_index(drop=True)\n",
    "\n",
    "    result_df.iloc[:,0] = (sample_id).astype(str)\n",
    "    cal = result_df['sample id'].str.contains('cal')\n",
    "    df_cal = result_df[cal]\n",
    "    df_final = result_df[np.logical_not(cal)]\n",
    "    df_final.index = range(len(df_final))\n",
    "\n",
    "    return df_final,df_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(drillhole):\n",
    "    \n",
    "    dict = pd.read_excel(f'{drillhole}/{drillhole}_sample_report.xlsx', sheet_name = None)\n",
    "    df_den = dict.get('den') \n",
    "    df_out = pd.concat([df_den['sample_id'],df_den['density']],axis=1)\n",
    "    df_out.rename(columns={'sample_id': 'sample id'}, inplace=True)\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_res (drillhole):\n",
    "\n",
    "    data_inventory = pd.read_excel(f'{drillhole}/{drillhole}_sample_report.xlsx', sheet_name = None)\n",
    "    df_record = data_inventory.get('ip_res')\n",
    "    sample_id = df_record['sample_id']\n",
    "\n",
    "    # meas_id_prop = df_record['measurement_id'].tolist()\n",
    "\n",
    "    os.chdir(f'{drillhole}/ip_res')\n",
    "\n",
    "    df_proc = pd.DataFrame()\n",
    "    \n",
    "    for dir in os.listdir():\n",
    "        os.chdir(dir+'/'+'kt20_0385'+'/'+'ip')\n",
    "        ip_record = pd.read_csv('ip_records.csv')\n",
    "        df_proc = pd.concat([df_proc, ip_record], ignore_index=True)\n",
    "\n",
    "        os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir+os.sep + os.pardir+os.sep + os.pardir))\n",
    "\n",
    "    os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir + os.sep + os.pardir))\n",
    "\n",
    "\n",
    "    nan_indices = df_record[df_record['measurement_id'].isna()].index\n",
    "\n",
    "    dic_nan = {str(i) : ['nan'] for i in df_proc.columns}\n",
    "    df_nan = pd.DataFrame(dic_nan)\n",
    "\n",
    "    \n",
    "    for index in nan_indices:\n",
    "        df_proc = pd.concat([df_proc.iloc[:index], df_nan, df_proc.iloc[index:]]).reset_index(drop=True)\n",
    "    result_df = df_proc.reset_index(drop=True)\n",
    "\n",
    "    result_df.iloc[:,0] = (sample_id).astype(str)\n",
    "    result_df.rename(columns={'Record_Id': 'sample id'}, inplace=True)\n",
    "    cal = result_df['sample id'].str.contains('cal')\n",
    "    df_cal = result_df[cal]\n",
    "    df_final = result_df[np.logical_not(cal)]\n",
    "    df_final.index = range(len(df_final))\n",
    "\n",
    "    return df_final,df_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,b = ip_res('TC-2531-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_utf8(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        enc = chardet.detect(f.read())\n",
    "\n",
    "    if enc['encoding'].lower() == 'utf-8':\n",
    "        print(f\"The file {input_file} is already in UTF-8 encoding.\")\n",
    "    else:\n",
    "        # Use a temporary DataFrame to read the CSV with the detected encoding\n",
    "        df = pd.read_csv(input_file, encoding=enc['encoding'])\n",
    "        \n",
    "        # Save the DataFrame with UTF-8 encoding\n",
    "        output_file = input_file.replace('.csv', '_utf8.csv')\n",
    "        df.to_csv(output_file, encoding='utf-8', sep=',',index=False)\n",
    "        print(f\"File converted and saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vel(drillhole,prop):\n",
    "\n",
    "    data_inventory = pd.read_excel(f'{drillhole}/{drillhole}_sample_report.xlsx', sheet_name = None)\n",
    "    df_record = data_inventory.get(prop)\n",
    "    sample_id = df_record['sample_id']\n",
    "\n",
    "    os.chdir(f'{drillhole}/{prop}')\n",
    "\n",
    "    df_proc = pd.DataFrame()\n",
    "\n",
    "    for dir in os.listdir():\n",
    "        os.chdir(dir)\n",
    "\n",
    "        \n",
    "        convert_csv_to_utf8('Punditlink Data.csv')\n",
    "\n",
    "        file = glob.glob('*.csv')\n",
    "\n",
    "        rawdata = open(file[len(file)-1], 'rb').read()\n",
    "        enc = chardet.detect(rawdata)\n",
    "        # df = pd.read_csv(list1[0],sep='\t', encoding = enc['encoding'],header = 14)\n",
    "\n",
    "        try:\n",
    "            df_data = pd.read_csv(file[len(file)-1],sep=',', encoding = enc['encoding'],header = 14)\n",
    "            df_data.columns = df_data.columns.str.strip()\n",
    "            record_id = df_data['ID']\n",
    "            a=[]\n",
    "            for i in range(1,len(df_data),25):\n",
    "                a.append(i)\n",
    "\n",
    "        except KeyError:\n",
    "            df_data = pd.read_csv(file[len(file)-1],sep='\\t', encoding = enc['encoding'],header = 10)\n",
    "            df_data.columns = df_data.columns.str.strip()\n",
    "            record_id = df_data['ID']\n",
    "            a=[]\n",
    "            for i in range(1,len(df_data),16):\n",
    "                a.append(i)\n",
    "  \n",
    "        df_vel = df_data.iloc[a]\n",
    "        df_proc = pd.concat([df_proc, df_vel], ignore_index=True)\n",
    "        os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir))\n",
    "\n",
    "    os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir + os.sep + os.pardir))\n",
    "\n",
    "    nan_indices = df_record[df_record['measurement_id'].isna()].index\n",
    "\n",
    "    dic_nan = {str(i) : ['nan'] for i in df_proc.columns}\n",
    "    df_nan = pd.DataFrame(dic_nan)\n",
    "\n",
    "    \n",
    "    for index in nan_indices:\n",
    "        df_proc = pd.concat([df_proc.iloc[:index], df_nan, df_proc.iloc[index:]]).reset_index(drop=True)\n",
    "    result_df = df_proc.reset_index(drop=True)\n",
    "\n",
    " \n",
    "    result_df.iloc[:,0] = (sample_id).astype(str)\n",
    "    result_df.rename(columns={'ID': 'sample id'}, inplace=True)\n",
    "    cal = result_df['sample id'].str.contains('cal')\n",
    "    df_cal = result_df[cal]\n",
    "    df_final = result_df[np.logical_not(cal)]\n",
    "    df_final.index = range(len(df_final))\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = vel('TC-2531-001','v_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_four_digits(number):\n",
    "    return float(number[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xrf(drillhole):\n",
    "        \n",
    "        os.chdir(f'{drillhole}/xrf')  \n",
    "\n",
    "        \n",
    "        df_proc = pd.DataFrame()\n",
    "\n",
    "        for dir in os.listdir():\n",
    "            os.chdir(dir)\n",
    "            file = glob.glob('*.csv')\n",
    "            try:\n",
    "                df_xrf = pd.read_csv(file[len(file)-1],skiprows=1)\n",
    "                info = df_xrf['info']\n",
    "            except KeyError:\n",
    "                df_xrf = pd.read_csv(file[len(file)-1])\n",
    "                info = df_xrf['info']\n",
    "            except KeyError:\n",
    "                df_xrf = pd.read_csv(file[len(file)-1],sep=';')\n",
    "                info = df_xrf['info']\n",
    "            except KeyError:\n",
    "                df_xrf = pd.read_csv(file[len(file)-1],sep=',')\n",
    "                info = df_xrf['info']\n",
    "               \n",
    "                \n",
    "            os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir))\n",
    "            df_proc = pd.concat([df_proc, df_xrf], ignore_index=True)\n",
    "\n",
    "        info = df_proc['info'].tolist()\n",
    "        \n",
    "        i = 0\n",
    "        drop_list = []\n",
    "\n",
    "        while i < len(info):\n",
    "            if isinstance(info[i], str) and info[i].startswith('cal'):\n",
    "                drop_list.append(i)\n",
    "            if isinstance(info[i], str) and info[i].startswith('std'):\n",
    "                drop_list.append(i)\n",
    "            if isinstance(info[i], str) and info[i].startswith('blk'):\n",
    "                drop_list.append(i)\n",
    "            i += 1\n",
    "        # print(drop_list)\n",
    "        new_df = df_proc.drop(df_proc.index[drop_list])\n",
    "\n",
    "        # display(new_df)\n",
    "\n",
    "        new_df.reset_index(inplace = True)\n",
    "        new_info = new_df['info'].tolist()\n",
    "        sampleid_list = []\n",
    "        # delete the last letter of info\n",
    "        for temp_info in new_info:\n",
    "            sampleid = str(temp_info)[:-1]\n",
    "            sampleid_list.append(sampleid)\n",
    "        # delete NaN & abcdef columns\n",
    "        new_df['info']=sampleid_list\n",
    "        new_df = new_df.dropna(axis=1, how='all') #drop all rows that have any NaN values\n",
    "        new_df =new_df.drop( ['Instrument Serial Num','Reading #','Date','Time','Method Name','Test Label','Collimation Status','Units'],axis=1)\n",
    "        # delete 'errors' columns\n",
    "        new_df1=new_df.filter(regex='Error')\n",
    "        new_df2=new_df.filter(regex='User')\n",
    "        # new_df1=new_df.filter(regex='Compound')\n",
    "        new_df = new_df.drop(new_df1,axis=1)\n",
    "        new_df = new_df.drop(new_df2,axis=1)\n",
    "        new_df = new_df.replace('<LOD','0.0')\n",
    "        new_df = new_df.replace('na',np.NaN)\n",
    "        new_df = new_df.replace('de',np.NaN)\n",
    "\n",
    "\n",
    "        new_df = pd.concat([new_df.iloc[:, :-1].apply(np.float64),new_df['info']],axis=1)\n",
    "        new_df['temp_info'] = new_df['info'].apply(extract_last_four_digits)\n",
    "\n",
    "        new_df1 = new_df.groupby(['temp_info'])[['Mg Concentration','Al Concentration','Si Concentration','P Concentration','S Concentration','K Concentration','Ca Concentration','Ti Concentration','V Concentration','Cr Concentration','Mn Concentration','Fe Concentration','Mg Concentration','Co Concentration','Ni Concentration','Cu Concentration','Zn Concentration','As Concentration','Se Concentration','Rb Concentration','Sr Concentration','Y Concentration','Zr Concentration','Nb Concentration','Mo Concentration','Ag Concentration','Cd Concentration','Sn Concentration','Sb Concentration','W Concentration','Au Concentration','Hg Concentration','Pb Concentration','Bi Concentration','Th Concentration','U Concentration','LE Concentration']].mean().reset_index()\n",
    "        new_df2 = new_df.groupby(['temp_info'])[['Mg Concentration','Al Concentration','Si Concentration','P Concentration','S Concentration','K Concentration','Ca Concentration','Ti Concentration','V Concentration','Cr Concentration','Mn Concentration','Fe Concentration','Mg Concentration','Co Concentration','Ni Concentration','Cu Concentration','Zn Concentration','As Concentration','Se Concentration','Rb Concentration','Sr Concentration','Y Concentration','Zr Concentration','Nb Concentration','Mo Concentration','Ag Concentration','Cd Concentration','Sn Concentration','Sb Concentration','W Concentration','Au Concentration','Hg Concentration','Pb Concentration','Bi Concentration','Th Concentration','U Concentration','LE Concentration']].std().reset_index()\n",
    "        new_df3 = new_df.groupby(['temp_info'])[['Mg Concentration','Al Concentration','Si Concentration','P Concentration','S Concentration','K Concentration','Ca Concentration','Ti Concentration','V Concentration','Cr Concentration','Mn Concentration','Fe Concentration','Mg Concentration','Co Concentration','Ni Concentration','Cu Concentration','Zn Concentration','As Concentration','Se Concentration','Rb Concentration','Sr Concentration','Y Concentration','Zr Concentration','Nb Concentration','Mo Concentration','Ag Concentration','Cd Concentration','Sn Concentration','Sb Concentration','W Concentration','Au Concentration','Hg Concentration','Pb Concentration','Bi Concentration','Th Concentration','U Concentration','LE Concentration']].median().reset_index()\n",
    "        new_df4 = new_df.groupby(['temp_info'])[['Mg Concentration','Al Concentration','Si Concentration','P Concentration','S Concentration','K Concentration','Ca Concentration','Ti Concentration','V Concentration','Cr Concentration','Mn Concentration','Fe Concentration','Mg Concentration','Co Concentration','Ni Concentration','Cu Concentration','Zn Concentration','As Concentration','Se Concentration','Rb Concentration','Sr Concentration','Y Concentration','Zr Concentration','Nb Concentration','Mo Concentration','Ag Concentration','Cd Concentration','Sn Concentration','Sb Concentration','W Concentration','Au Concentration','Hg Concentration','Pb Concentration','Bi Concentration','Th Concentration','U Concentration','LE Concentration']].max().reset_index()\n",
    "        new_df5 = new_df.groupby(['temp_info'])[['Mg Concentration','Al Concentration','Si Concentration','P Concentration','S Concentration','K Concentration','Ca Concentration','Ti Concentration','V Concentration','Cr Concentration','Mn Concentration','Fe Concentration','Mg Concentration','Co Concentration','Ni Concentration','Cu Concentration','Zn Concentration','As Concentration','Se Concentration','Rb Concentration','Sr Concentration','Y Concentration','Zr Concentration','Nb Concentration','Mo Concentration','Ag Concentration','Cd Concentration','Sn Concentration','Sb Concentration','W Concentration','Au Concentration','Hg Concentration','Pb Concentration','Bi Concentration','Th Concentration','U Concentration','LE Concentration']].min().reset_index()\n",
    "        new_df1.columns = new_df1.columns.str.replace('Concentration', 'Mean')\n",
    "        # new_df1.columns = new_df1.replace('Concentration', 'Mean')\n",
    "        new_df2.columns = new_df2.columns.str.replace('Concentration', 'Std')\n",
    "        new_df3.columns = new_df3.columns.str.replace('Concentration', 'Median')\n",
    "        new_df4.columns = new_df4.columns.str.replace('Concentration', 'Max')\n",
    "        new_df5.columns = new_df5.columns.str.replace('Concentration', 'Min')\n",
    "        new_df6 = pd.concat([new_df1,new_df2,new_df3,new_df4,new_df5],axis=1)\n",
    "        new_df7 = new_df6.sort_index(axis=1)\n",
    "        new_df7 = new_df7.loc[:, ~new_df7.columns.duplicated()]\n",
    "        new_df7 = new_df7.iloc[:, :-1]\n",
    "        new_df7['info']=np.unique(sampleid_list)\n",
    "        df_final = new_df7 \n",
    "        \n",
    "        df_final.rename(columns={'info': 'sample id'}, inplace=True)\n",
    "\n",
    "        os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir+ os.sep + os.pardir))\n",
    "\n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = xrf('campaign03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.normpath(os.getcwd() + os.sep + os.pardir+ os.sep + os.pardir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysis140323",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
